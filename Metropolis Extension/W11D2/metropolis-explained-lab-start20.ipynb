{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">INFO 6105 Data Science Eng Methods and Tools, Lecture 11 Day 2</div>\n",
    "<div style=\"text-align: right\">Dino Konstantopoulos, 19 November 2020</div>\n",
    "\n",
    "# Metropolis lab\n",
    "\n",
    "Last lecture we studied the [Metropolis algorithm](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) and used it to model some data we generated. We looked at the histogram of the data, decided to model it with a gaussian pdf, then used the Metropolis algorithm to \"*predict*\" the value of $\\mu$. \n",
    "\n",
    "So what we're going to do today is to divide you in groups and ask you to do the same thing, but to predict/estimate the **standard deviation** instead!\n",
    "\n",
    "Read the formulas and the code, and fill-in the ellipses (`...`)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import matplotlib as mpl   \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data generation\n",
    "\n",
    "We generate 30,000 samples from a normal distribution with $\\mu$ = 10, and $\\sigma$= 3, time time. \n",
    "\n",
    "But let's say we can only observe 1000 of them. \n",
    "\n",
    "Our goal: Use Bayesian estimation to build a **model** from the 1000 observations, then use the model to reconstruct (a simulation of) the 30,000 samples.\n",
    "\n",
    "Let's start by plotting the histogram of these \"*observed*\" 1000 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=lambda mu, sig, t: np.random.normal(mu, sig, t)\n",
    "\n",
    "# fill in the ...!\n",
    "#Form a population of 30,000 individuals, with average=10 and sigma=3\n",
    "population = model(...)\n",
    "\n",
    "#Assume we are only able to observe 1,000 of these individuals, sampled randomly amongst the 30000.\n",
    "observation = population[np.random.randint(...)]\n",
    "\n",
    "# Compute the histogram of these 1,000 observed individuals\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist( observation,bins=35 ,)\n",
    "ax.set_xlabel(\"Value\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Figure 1: Distribution of 1000 observations sampled from a population of 30,000 with $\\mu$=10, $\\sigma$=3\")\n",
    "\n",
    "# fill in the ...!\n",
    "#What is the inferred mu and sigma from these 1,000 observations?\n",
    "mu_obs=...\n",
    "sig_obs=...\n",
    "mu_obs, sig_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Which parameter to model\n",
    "\n",
    "Our model parameter vector $\\theta$ is made up of two values: $[\\mu,\\sigma]$. Let's assume  $\\mu$ is a constant, $\\mu = \\mu_{obs}$.\n",
    "\n",
    "We don't know *how sure* we should be of the 1000-deduced value above `sig_obs` for the standard deviation.\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/suspicious2.png \" width=200 />\n",
    "</left> \n",
    "\n",
    "We would like to find a distribution for $\\sigma_{obs}$ using the 1000 observed samples. \n",
    "\n",
    "Some of you may say, \"but, professor, *there is a formula for computing the standard deviation* $\\sigma$, and we used it above\".\n",
    "\n",
    "Actually, ***all of you need to know how to compute standard deviation from $n$ observations $d_i$ with mean $\\mu$***:\n",
    "\n",
    "$$\\sigma=\\sqrt{\\frac{1}{n}\\sum_i^n(d_i-\\mu)^2}$$\n",
    "\n",
    "Note however, that this only makes sense for a single-hump distribution, and that we are not trying to find *a* value for $\\sigma$, but rather, we are trying to compute a distribution of all possible values of $\\sigma$.\n",
    "\n",
    "## Step 3: Define the pdf for the prior and pdf for the likelihood\n",
    "\n",
    "From the figure above, we can see that the data is **normally distributed**. The mean can be easily computed by taking the average of the values of the 1000 samples. By doing that, we get for example $\\mu_{obs}=9.8$.\n",
    "\n",
    "To predict the standard deviation, we need to model it with a pdf, let's pick a simple one: Why, the normal distribution $f$ of course!\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/inspector-gadget.jpg\" width=200 />\n",
    "</left> \n",
    "\n",
    "\\begin{equation} \\sigma_{new} = f \\sim N(\\mu=\\sigma_{current},\\; \\sigma'=1) \\end{equation}\n",
    "\n",
    "Note that $\\sigma'$ is unrelated to $\\sigma_{new}$ and $\\sigma_{current}$. It simply specifies the standard deviation of our parameter space. It can be any value desired. It only affects the convergence time of the algorithm.\n",
    "\n",
    "Also, note that we are modeling the standard deviation using the *mean* of a normal distribution..\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/oopsie-smiley2.png\" width=200 />\n",
    "</left> \n",
    "\n",
    "We don't have any preferences values that $\\sigma_{new}$ and $\\sigma_{current}$ can take, but they should be positive! Why? Intuitively, the standard deviation measures **dispersion**. Dispersion is a distance, and distances cannot be negative.\n",
    "\n",
    "Mathematically, $\\sigma=\\sqrt{\\dfrac{1}{n}\\sum_i^n(d_i-\\mu)^2}$, and the square root of a number cannot be negative. We should strictly enforce this in the prior.\n",
    "\n",
    "Our likelihood $f$ is the following pdf, for each data point $d_i$ in the data D:\n",
    "\n",
    "\\begin{equation} f(d_i\\;|\\; \\mu,\\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\dfrac{(d_i-\\mu)^2}{2\\sigma^2}} \\end{equation}\n",
    "\n",
    "## Step 4: Define when we accept or reject $\\sigma_{new}$: \n",
    "Our Metropolis algorithm says that we accept a new guess $\\sigma_{new}$ from the old position $\\sigma_{current}$ if:\n",
    "\n",
    "$\\dfrac{\\text{Likelihood}(D \\;|\\; \\mu_{obs},\\sigma_{new})\\; * \\; \\text{prior}(\\mu_{obs},\\sigma_{new})}{\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{current})\\;*\\;\\text{prior}(\\mu_{obs},\\sigma_{current})} \\;\\;>\\;\\; 1     \\quad \\quad \\quad \\quad \\quad      (1)$\n",
    "\n",
    "However, if this ratio is smaller or equal to 1, then we compare it to a uniformly generated random number in the closed set [0,1] (the amount by which our boyfriend/girlfriend's dinner is *better* than ours ;-).\n",
    "\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/wicked.png\" width=200 />\n",
    "</left> \n",
    "\n",
    "If the ratio is larger than the random number, we accept $\\sigma_{new}$, otherwise we reject it.\n",
    "\n",
    "*Note: Since we will be computing this ratio to decide which parameters should be accepted, make sure that the adopted likelihood is proportional to the posterior itself, $P(\\sigma\\;|\\; D,\\mu)$, which in our case is true.*\n",
    "\n",
    "\n",
    "## Step 5: Acceptance condition derivation:\n",
    "\n",
    "The total likelihood for a set of observations $D$ is: $\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) = \\prod_i^n f(d_i\\;|\\;\\mu_{obs},\\sigma_{new}) $\n",
    "\n",
    "This is the product of all our data points. Just like if I asked you what is the likelihood I will draw *four* heads in a row if I flip a coin, you would say it's $\\prod_i^n f$ with $f = 0.5$, so $0.5 * 0.5 * 0.5 * 0.5$.\n",
    "\n",
    "In our case, we will **log** both the prior and the likelihood function. Why log? Simply because it helps with \n",
    "**numerical stability**, i.e. multiplying thousands of small values (probabilities, likelihoods, etc..) can cause an **underflow** in  system memory (like it did in our previous notebook where we had 0s for our total likelihood), and the **log** is a perfect solution because it transforms multiplications to **additions** and small positive numbers into not-so-small negative numbers.\n",
    "\n",
    "Therefore our acceptance condition from equation $(1)$ above:\n",
    "Accept $\\sigma_{new}$ if:\n",
    "\n",
    "$$\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) * \\text{prior}(\\mu_{obs},\\sigma_{new})) > \\\\ \\text{Likelihood}(D\\;|\\:\\mu_{obs},\\sigma_{current}) *  \\text{prior}(\\mu_{obs},\\sigma_{current}))$$\n",
    "\n",
    "After taking the **log** of equation (1), since **log** is a [monotonic](https://en.wikipedia.org/wiki/Monotonic_function) function:\n",
    "\n",
    "$$Log(\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) * \\text{prior}(\\mu_{obs},\\sigma_{new}))) > \\\\ Log(\\text{Likelihood}(D\\;|\\:\\mu_{obs},\\sigma_{current}) *  \\text{prior}(\\mu_{obs},\\sigma_{current})))$$\n",
    "\n",
    "Also, since $Log(a * b) = Log(a) + Log(b)$, our condition becomes:\n",
    "\n",
    "$$Log(\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new})) + Log(\\text{prior}(\\mu_{obs},\\sigma_{new})) - (Log(\\text{Likelihood}(D\\;|\\:\\mu_{obs},\\sigma_{current})) + Log(prior(\\mu_{obs},\\sigma_{current})))\\;>\\;0$$\n",
    "\n",
    "By plugging in $\\text{Likelihood}(D\\;|\\;\\mu_{obs},\\sigma_{new}) = \\prod_i^n f(d_i\\;|\\;\\mu_{obs},\\sigma_{new}) $, our condition becomes:\n",
    "\n",
    " $$\\sum_i^nLog(f(d_i\\;|\\;\\mu_{obs},\\sigma_{new})) + Log(\\text{prior}(\\mu_{obs},\\sigma_{new})) - \\sum_i^nLog(f(d_i\\;|\\;\\mu_{obs},\\sigma_{current}))-Log(\\text{prior}(\\mu_{obs},\\sigma_{current}))>0$$\n",
    " \n",
    " $\\quad$\n",
    " \n",
    " \n",
    "By plugging in \\begin{equation} f(d_i\\;|\\; \\mu,\\sigma^2) = \\dfrac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\dfrac{(d_i-\\mu)^2}{2\\sigma^2}} \\end{equation}\n",
    "\n",
    "Our condition becomes: \n",
    "  \n",
    "$$\\sum_i^n -Log(\\sigma_{new}\\sqrt{2\\pi})-\\dfrac{(d_i-\\mu_{obs})^2}{2\\sigma_{new}^2} \\;\\;+\\;\\; Log(prior(\\mu_{obs},\\sigma_{new})) \\quad > \\\\\n",
    "\\quad \\sum_i^n -Log(\\sigma_{current}\\sqrt{2\\pi})-\\dfrac{(d_i-\\mu_{obs})^2}{2\\sigma_{current}^2} \\;\\;+\\;\\; Log(prior(\\mu_{obs},\\sigma_{current})) \\quad \\quad  (2)$$\n",
    "\n",
    "So (2) will be our acceptance condition:\n",
    "- If true, we will always accept $\\sigma_{new}$ as our next value in the chain\n",
    "- If false, ***we will only accept it accroding to a probability equal to the ratio of the nominators*** (like in the lecture notebook).\n",
    "\n",
    "So let's write some code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The tranistion model defines how to move from sigma_current to sigma_new\n",
    "transition_model = lambda x: [x[0], np.random.normal(x[1], 0.5, (1,))]\n",
    "\n",
    "def prior(x):\n",
    "    #x[0] = mu, x[1]=sigma (new or current)\n",
    "    #returns 1 for all valid values of sigma. Log(1) =0, so it does not affect the summation.\n",
    "    #returns 0 for all invalid values of sigma (<=0). Log(0)=-infinity, and Log(negative number) is undefined.\n",
    "    #It makes the new sigma infinitely unlikely.\n",
    "    if(x[1] <= 0):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#Computes the likelihood of the data given a sigma (new or current) according to equation (2)\n",
    "def manual_log_like_normal(x, data):\n",
    "    #x[0]=mu, x[1]=sigma (new or current)\n",
    "    #data = the observation\n",
    "    return np.sum(-np.log(x[1] * np.sqrt(2* np.pi) )-((data-x[0])**2) / (2*x[1]**2))\n",
    "\n",
    "#Same as manual_log_like_normal(x,data), but using scipy implementation. It's pretty slow.\n",
    "def log_lik_normal(x, data):\n",
    "    #x[0]=mu, x[1]=sigma (new or current)\n",
    "    #data = the observation\n",
    "    return np.sum(np.log(scipy.stats.norm(x[0], x[1]).pdf(data)))\n",
    "\n",
    "\n",
    "#Defines whether to accept or reject the new sample\n",
    "def acceptance(x, x_new):\n",
    "    if x_new > x:\n",
    "        # we are always right!\n",
    "        return True\n",
    "    else:\n",
    "        # We'll say that our boyfriend/girlfriend is sometimes right, using a randon number generator\n",
    "        accept=np.random.uniform(0,1)\n",
    "        # Since we did a log likelihood, we need to exponentiate in order to compare to the random number\n",
    "        # less likely x_new are less likely to be accepted\n",
    "        return (accept < (np.exp(x_new - x)))\n",
    "\n",
    "\n",
    "# fill in the ...!\n",
    "def metropolis_hastings(likelihood_computer, prior, transition_model, param_init, iterations, data,acceptance_rule):\n",
    "    # likelihood_computer(x,data): returns the likelihood that these parameters generated the data\n",
    "    # transition_model(x): a function that draws a sample from a symmetric distribution and returns it\n",
    "    # param_init: a starting sample\n",
    "    # iterations: number of accepted to generated\n",
    "    # data: the data that we wish to model\n",
    "    # acceptance_rule(x,x_new): decides whether to accept or reject the new sample\n",
    "    x = param_init\n",
    "    accepted = []\n",
    "    rejected = []   \n",
    "    for i in range(iterations):\n",
    "        x_new =  transition_model(x)    \n",
    "        x_lik = likelihood_computer(x, data)\n",
    "        x_new_lik = likelihood_computer(x_new, data) \n",
    "        if ...:            \n",
    "            x = x_new\n",
    "            accepted.append(x_new)\n",
    "        else:\n",
    "            rejected.append(x_new)            \n",
    "                \n",
    "    return np.array(accepted), np.array(rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Run the algorithm with initial parameters and collect accepted and rejected samples from 50,000 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted, rejected = metropolis_hastings(manual_log_like_normal, prior, transition_model, [mu_obs,0.1], 50000, \n",
    "                                         observation, acceptance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rejected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm accepted 8803 samples (which might be different on each new run).\n",
    "\n",
    "The last 10 samples contain the following  values for $\\sigma$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted[-10:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accepted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot accepted and rejected values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "\n",
    "ax.plot( rejected[0:50,1], 'rx', label='Rejected',alpha=0.5)\n",
    "ax.plot( accepted[0:50,1], 'b.', label='Accepted',alpha=0.5)\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax.set_ylabel(\"$\\sigma$\")\n",
    "ax.set_title(\"Figure 2: MCMC sampling for $\\sigma$ with Metropolis-Hastings. First 50 samples are shown.\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "to_show=-accepted.shape[0]\n",
    "ax2.plot( rejected[to_show:,1], 'rx', label='Rejected',alpha=0.5)\n",
    "ax2.plot( accepted[to_show:,1], 'b.', label='Accepted',alpha=0.5)\n",
    "ax2.set_xlabel(\"Iteration\")\n",
    "ax2.set_ylabel(\"$\\sigma$\")\n",
    "ax2.set_title(\"Figure 3: MCMC sampling for $\\sigma$ with Metropolis-Hastings. All samples are shown.\")\n",
    "ax2.grid()\n",
    "ax2.legend()\n",
    "\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "accepted.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, starting from an initial σ of 0.1, the algorithm converged pretty quickly to the expected value of 3.\n",
    "<br />\n",
    "<left>\n",
    "<img src=\"ipynb.images/smiley-wow.png\" width=200 />\n",
    "</left> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We consider the initial 25% of the values of $\\sigma$ to be \"burn-in\", so we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accepted) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.00912569])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill-in the ...\n",
    "accepted[...:,1].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's visualize the trace of  $\\sigma$ and the histogram of the trace\n",
    "(this will take a few minutes).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show=int(-0.75*accepted.shape[0])\n",
    "hist_show=int(-0.75*accepted.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(1,2,1)\n",
    "ax.plot(accepted[show:,1])\n",
    "ax.set_title(\"Figure 4: Trace for $\\sigma$\")\n",
    "ax.set_ylabel(\"$\\sigma$\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "ax = fig.add_subplot(1,2,2)\n",
    "ax.hist(accepted[hist_show:,1], bins=20,density=True)\n",
    "ax.set_ylabel(\"Frequency (normed)\")\n",
    "ax.set_xlabel(\"$\\sigma$\")\n",
    "ax.set_title(\"Figure 5: Histogram of $\\sigma$\")\n",
    "fig.tight_layout()\n",
    "\n",
    "#ax.grid(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most likely value for $\\sigma$ is .... This is a bit more than the original value of 3.0. \n",
    "\n",
    "The difference is due to us observing only 3.33% of the original population (1,000 out of 30,000) \n",
    "\n",
    "# Prediction/Inference\n",
    "First, we average the last 75% of accepted samples of σ, and we generate 30,000 random individuals from a normal distribution with μ=9.8 and σ=3.05 (the average of the last 75% of accepted samples) which is actually better than the most likely value of 3.1.\n",
    "\n",
    "Then we compare against the original data of 30,000 individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.070675970330738 3.017197436380267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2b058d4c390>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu=accepted[show:,0].mean()\n",
    "sigma=accepted[show:,1].mean()\n",
    "print(mu, sigma)\n",
    "model = lambda t,mu,sigma:np.random.normal(mu,sigma,t)\n",
    "observation_gen=model(population.shape[0],mu,sigma)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist( observation_gen,bins=70 ,label=\"Predicted distribution of 30,000 individuals\")\n",
    "ax.hist( population,bins=70 ,alpha=0.5, label=\"Original values of the 30,000 individuals\")\n",
    "ax.set_xlabel(\"Mean\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Figure 6: Posterior distribution of predicitons\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good fit? Our model, generated from a small 1,000 datapoint sample of 30,000 observations, succeeds in modelling all 30,000 observations.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "That is how probabilistic programming works. Assume distributions for your model parameters, and let computers run simulations without bias!\n",
    "\n",
    "Of course, if you *assumed* bias in your model shapes, then you may also get junk values. But actually we see that, most of the time*, probabilstic programs actually can *correct our initial mistakes* and give us not just most probably parameter values, but actual parameter pdfs that lokk *nothing like what we started with*.\n",
    "\n",
    ">That is computers erasing our bias! :-)\n",
    "\n",
    "The great thing about probabilistic programming is that you only need to write down the model and then run it. The simplest MCMC algorithm, [Metropolis](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm), is very simple. There is no need to compute evidence (denominator), or ensure constraining mathematical properties."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
